{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Takes as input the (768, 768, 64) images from `dataset-1`.\n",
    "- It takes 4^3 average across the 2D, and average across the depth\n",
    "- This results in a (192, 192, 16) volume.\n",
    "- I have a image that is of shape (1, 192, 192, 16). Divide the image and the labels into 32 patches: (1, 8*24, 8*24, 16) → (24, 24, 1024) → (576, 1024). \n",
    "- Pass the image into a transformer architecture with 6 layers, which will return (576, 1) tensor, which corresponds to probabilities of pixel occuring. You should also pass the “positional embeddings”, which should be of dimension 1024."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4^3 3D average \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def avg_3d(volume):\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    volume_tensor = torch.tensor(volume, dtype=torch.float32)\n",
    "\n",
    "    # Add batch and channel dimensions to the tensor\n",
    "    volume_tensor = volume_tensor.permute(0, 3, 1, 2)  # Reorder dimensions to (batch, channels, height, width)\n",
    "\n",
    "    # Create the 3D average pooling layer with the appropriate kernel size and stride values\n",
    "    avg_pool = nn.AvgPool3d(kernel_size=4, stride=4, padding=0)\n",
    "\n",
    "    # Apply the average pooling layer to the input tensor\n",
    "    with torch.no_grad():\n",
    "        filtered_volume_tensor = avg_pool(volume_tensor)\n",
    "\n",
    "    # Convert the output tensor back to a numpy array\n",
    "    filtered_volume = filtered_volume_tensor.permute(0, 2, 3, 1)  # Reorder dimensions back to (batch, height, width, channels)\n",
    "    \n",
    "    return filtered_volume\n",
    "\n",
    "\n",
    "# Example volume\n",
    "volume = np.random.rand(1, 768, 768, 64)\n",
    "volume_avgd = avg_3d(volume)\n",
    "# %time avg_3d(volume) # 85 ms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 192, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_avgd.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take random 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 192, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_avgd_new = volume_avgd.permute(3, 1, 2, 0)\n",
    "# take 3 random numbers between 0 and 16, without replacement\n",
    "random_numbers = np.random.choice(16, 3, replace=False)\n",
    "volume_avgd_new = volume_avgd_new[random_numbers, :, :, :]\n",
    "# Repermute\n",
    "volume_avgd_new = volume_avgd_new.permute(3, 1, 2, 0)\n",
    "volume_avgd_new.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def reshape_img(image):\n",
    "    # Calculate the size of each patch\n",
    "    patch_size = 24\n",
    "\n",
    "    B, H, W, C = image.shape\n",
    "    image = image.reshape(B, H // patch_size, patch_size, W // patch_size, patch_size, C) # (B, 8, 24, 8, 24, 16)\n",
    "    image = image.permute(0, 2, 4, 1, 3, 5) # (B, 24, 24, 8, 8, 16)\n",
    "    image = image.reshape(B, patch_size, patch_size, -1) # (B, 24, 24, 1024)\n",
    "    image = image.reshape(B, -1, 1024) # (B, 576, 1024)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Example image\n",
    "image = np.random.rand(2, 192, 192, 16)\n",
    "image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "\n",
    "image = reshape_img(image)\n",
    "\n",
    "print(image.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric competition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0point5_score(output, target):\n",
    "    # Flatten the output and target tensors\n",
    "    output = output.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    # Convert the output to binary values 0 and 1\n",
    "    output = (output > 0.5).float()\n",
    "    \n",
    "    # Calculate the precision and recall\n",
    "    tp = torch.sum(output * target)\n",
    "    fp = torch.sum(output * (1 - target))\n",
    "    fn = torch.sum((1 - output) * target)\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    \n",
    "    # Calculate the F0.5 score\n",
    "    beta = 0.5\n",
    "    f0point5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-7)\n",
    "    return f0point5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: ResNet50Seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet50Seg(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, class_one_weight=1):\n",
    "        super(ResNet50Seg, self).__init__()\n",
    "        \n",
    "        self.class_one_weight = class_one_weight\n",
    "        \n",
    "        # Load the pre-trained ResNet50 model\n",
    "        self.resnet50 = models.resnet50()\n",
    "        \n",
    "        # Replace the final layer to output 1024 channel instead of 1000\n",
    "        self.resnet50.fc = nn.Linear(2048, 1024)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.resnet50(x)\n",
    "        # Apply sigmoid\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        out = {}\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            precision = None\n",
    "            accuracy = None\n",
    "            f0point5 = None\n",
    "        else:\n",
    "            # Calculate the loss\n",
    "            \n",
    "            # Calculate class weights based on the imbalance\n",
    "            class_weights = torch.tensor([1.0, self.class_one_weight]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "            # Instantiate the loss function\n",
    "            loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "            \n",
    "            # Flatten the targets tensor\n",
    "            targets = targets.reshape(-1, 1024)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_function(x, targets)\n",
    "            \n",
    "            # Calculate the accuracy: number of correctly predicted pixel / total number of pixels\n",
    "            # Convert the predictions to binary values 0 and 1\n",
    "            predictions = (x > 0.5).float()\n",
    "            # Calculate the accuracy\n",
    "            accuracy = (predictions == targets).float().mean()\n",
    "            \n",
    "            # Calculate the precision\n",
    "            tp = torch.sum(predictions * targets)   \n",
    "            fp = torch.sum(predictions * (1 - targets))\n",
    "            fn = torch.sum((1 - predictions) * targets)\n",
    "            precision = tp / (tp + fp + 1e-7)\n",
    "            \n",
    "            # Calculate F0.5 score\n",
    "            f0point5 = f0point5_score(predictions, targets) \n",
    "        \n",
    "        out = {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": x,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"f0point5\": f0point5\n",
    "        }\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50Seg(in_channels=3, out_channels=1)\n",
    "x = torch.randn(1, 3, 192, 192)  # input image\n",
    "output = model(x)\n",
    "output['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.606208"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 1e6 parameters\n",
    "sum(p.numel() for p in model.parameters()) / 1e6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode: UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 192, 192])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=32, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "# example image\n",
    "image = torch.randn(1, 32, 192, 192)  # input image\n",
    "\n",
    "output = model(image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 256, 256]           9,216\n",
      "       BatchNorm2d-2         [-1, 32, 256, 256]              64\n",
      "              ReLU-3         [-1, 32, 256, 256]               0\n",
      "            Conv2d-4         [-1, 32, 256, 256]           9,216\n",
      "       BatchNorm2d-5         [-1, 32, 256, 256]              64\n",
      "              ReLU-6         [-1, 32, 256, 256]               0\n",
      "         MaxPool2d-7         [-1, 32, 128, 128]               0\n",
      "            Conv2d-8         [-1, 64, 128, 128]          18,432\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "           Conv2d-11         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-12         [-1, 64, 128, 128]             128\n",
      "             ReLU-13         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15          [-1, 128, 64, 64]          73,728\n",
      "      BatchNorm2d-16          [-1, 128, 64, 64]             256\n",
      "             ReLU-17          [-1, 128, 64, 64]               0\n",
      "           Conv2d-18          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-19          [-1, 128, 64, 64]             256\n",
      "             ReLU-20          [-1, 128, 64, 64]               0\n",
      "        MaxPool2d-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22          [-1, 256, 32, 32]         294,912\n",
      "      BatchNorm2d-23          [-1, 256, 32, 32]             512\n",
      "             ReLU-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-26          [-1, 256, 32, 32]             512\n",
      "             ReLU-27          [-1, 256, 32, 32]               0\n",
      "        MaxPool2d-28          [-1, 256, 16, 16]               0\n",
      "           Conv2d-29          [-1, 512, 16, 16]       1,179,648\n",
      "      BatchNorm2d-30          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-31          [-1, 512, 16, 16]               0\n",
      "           Conv2d-32          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-34          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-35          [-1, 256, 32, 32]         524,544\n",
      "           Conv2d-36          [-1, 256, 32, 32]       1,179,648\n",
      "      BatchNorm2d-37          [-1, 256, 32, 32]             512\n",
      "             ReLU-38          [-1, 256, 32, 32]               0\n",
      "           Conv2d-39          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 32, 32]             512\n",
      "             ReLU-41          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-42          [-1, 128, 64, 64]         131,200\n",
      "           Conv2d-43          [-1, 128, 64, 64]         294,912\n",
      "      BatchNorm2d-44          [-1, 128, 64, 64]             256\n",
      "             ReLU-45          [-1, 128, 64, 64]               0\n",
      "           Conv2d-46          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 64, 64]             256\n",
      "             ReLU-48          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-49         [-1, 64, 128, 128]          32,832\n",
      "           Conv2d-50         [-1, 64, 128, 128]          73,728\n",
      "      BatchNorm2d-51         [-1, 64, 128, 128]             128\n",
      "             ReLU-52         [-1, 64, 128, 128]               0\n",
      "           Conv2d-53         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-54         [-1, 64, 128, 128]             128\n",
      "             ReLU-55         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-56         [-1, 32, 256, 256]           8,224\n",
      "           Conv2d-57         [-1, 32, 256, 256]          18,432\n",
      "      BatchNorm2d-58         [-1, 32, 256, 256]              64\n",
      "             ReLU-59         [-1, 32, 256, 256]               0\n",
      "           Conv2d-60         [-1, 32, 256, 256]           9,216\n",
      "      BatchNorm2d-61         [-1, 32, 256, 256]              64\n",
      "             ReLU-62         [-1, 32, 256, 256]               0\n",
      "           Conv2d-63          [-1, 1, 256, 256]              33\n",
      "================================================================\n",
      "Total params: 7,771,393\n",
      "Trainable params: 7,771,393\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.00\n",
      "Forward/backward pass size (MB): 404.00\n",
      "Params size (MB): 29.65\n",
      "Estimated Total Size (MB): 441.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (32, 256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 192, 192])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=False)\n",
    "\n",
    "# example image\n",
    "image = torch.randn(1, 192, 192, 16)  # input image\n",
    "\n",
    "# Make 3 dense layers that will make the number of channels from 16 to 3\n",
    "dense1 = nn.Linear(16, 16)\n",
    "dense2 = nn.Linear(16, 8)\n",
    "dense3 = nn.Linear(8, 3)\n",
    "\n",
    "image = image.reshape(-1, 16)  # Flatten the image\n",
    "image = dense1(image)  # Apply the first dense layer\n",
    "# relu\n",
    "image = torch.max(image, torch.zeros_like(image))\n",
    "image = dense2(image)  # Apply the second dense layer\n",
    "image = torch.max(image, torch.zeros_like(image))\n",
    "image = dense3(image)  # Apply the third dense layer\n",
    "image = torch.max(image, torch.zeros_like(image))\n",
    "\n",
    "# reshape to (1, 192, 192, 3)\n",
    "image = image.reshape(1, 192, 192, 3)\n",
    "# permute to (1, 3, 192, 192)\n",
    "image = image.permute(0, 3, 1, 2)\n",
    "\n",
    "output = model(image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  5,  8, 10, 15, 18, 20, 26, 28, 29, 31, 32, 35, 42, 48, 60])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.random.choice(64, 16, replace=False)\n",
    "indices = np.sort(indices)\n",
    "indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Unet (tests for a script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_width=384, in_channels=16, out_channels=1, init_features=64, class_one_weight=1):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.img_width = img_width\n",
    "        self.class_one_weight = class_one_weight\n",
    "        \n",
    "        # Load the pre-trained UNet model\n",
    "        self.unet = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=in_channels, out_channels=out_channels, init_features=init_features, pretrained=False)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.unet(x)\n",
    "        # Convert the predictions to binary values 0 and 1\n",
    "        predictions = (x > 0.5).float()\n",
    "        \n",
    "        out = {}\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            precision = None\n",
    "            accuracy = None\n",
    "            f0point5 = None\n",
    "        else:\n",
    "            # Calculate the loss\n",
    "            \n",
    "            # Calculate class weights based on the imbalance\n",
    "            class_weights = torch.tensor([1.0, self.class_one_weight]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "            # Instantiate the loss function\n",
    "            loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "            \n",
    "            # Flatten the targets and x tensor\n",
    "            targets = targets.reshape(-1, int(self.img_width ** 2))\n",
    "            x = x.reshape(-1, int(self.img_width ** 2))\n",
    "            predictions_flat = predictions.reshape(-1, int(self.img_width ** 2))\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_function(x, targets)\n",
    "            \n",
    "            # Calculate the accuracy: number of correctly predicted pixel / total number of pixels\n",
    "            # Calculate the accuracy\n",
    "            accuracy = (predictions_flat == targets).float().mean()\n",
    "            \n",
    "            # Calculate the precision\n",
    "            tp = torch.sum(predictions_flat * targets)   \n",
    "            fp = torch.sum(predictions_flat * (1 - targets))\n",
    "            fn = torch.sum((1 - predictions_flat) * targets)\n",
    "            precision = tp / (tp + fp + 1e-7)\n",
    "            \n",
    "            # Calculate F0.5 score\n",
    "            f0point5 = f0point5_score(predictions_flat, targets) \n",
    "        \n",
    "        out = {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": x,\n",
    "            \"predictions\": predictions,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"f0point5\": f0point5\n",
    "        }\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': None,\n",
       " 'logits': tensor([[[[0.3822, 0.4928, 0.5304,  ..., 0.5390, 0.5003, 0.4663],\n",
       "           [0.3897, 0.4987, 0.3285,  ..., 0.4702, 0.4966, 0.4246],\n",
       "           [0.4077, 0.5033, 0.4310,  ..., 0.5758, 0.3895, 0.4993],\n",
       "           ...,\n",
       "           [0.3387, 0.4023, 0.4429,  ..., 0.4360, 0.3875, 0.4244],\n",
       "           [0.4495, 0.4083, 0.4170,  ..., 0.3509, 0.3790, 0.5283],\n",
       "           [0.2891, 0.3429, 0.4832,  ..., 0.3691, 0.3413, 0.3569]]]],\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " 'predictions': tensor([[[[0., 0., 1.,  ..., 1., 1., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 0.,  ..., 1., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " 'accuracy': None,\n",
       " 'precision': None,\n",
       " 'f0point5': None}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(in_channels=16, out_channels=1)\n",
    "# example image\n",
    "image = torch.randn(1, 16, 192, 192)  # input image\n",
    "\n",
    "output = model(image)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.5615, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'logits': tensor([[0.5646, 0.5351, 0.4979,  ..., 0.5414, 0.4981, 0.4839]],\n",
       "        grad_fn=<ReshapeAliasBackward0>),\n",
       " 'predictions': tensor([[[[1., 1., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 0.,  ..., 1., 0., 0.],\n",
       "           ...,\n",
       "           [1., 1., 0.,  ..., 1., 1., 1.],\n",
       "           [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [0., 0., 1.,  ..., 1., 0., 0.]]]]),\n",
       " 'accuracy': tensor(0.),\n",
       " 'precision': tensor(0.7952),\n",
       " 'f0point5': tensor(0.8108)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(img_width=192, in_channels=16, out_channels=1)\n",
    "# example image\n",
    "image = torch.randn(1, 16, 192, 192)  # input image\n",
    "targets = torch.randn(1, 1, 192, 192)  # input image\n",
    "targets = torch.abs(targets)\n",
    "\n",
    "output = model(image, targets)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2220],\n",
       "         [2.9907],\n",
       "         [1.2412],\n",
       "         [1.7058],\n",
       "         [0.6931]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate class weights based on the imbalance\n",
    "class_weights = torch.tensor([1.0, 5.0]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='none')\n",
    "\n",
    "# Example output and targets\n",
    "out = [[0.1], [0.2], [0.9], [0.9], [0]]\n",
    "tar = [[1], [1], [0], [1], [0]]\n",
    "output = torch.tensor([out], dtype=torch.float32)\n",
    "targets = torch.tensor([tar], dtype=torch.float32)\n",
    "loss_function(output, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9705)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate class weights based on the imbalance\n",
    "class_weights = torch.tensor([1.0, 5.0]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "# Example output and targets\n",
    "out = [[0.1], [0.2], [0.9], [0.9], [0]]\n",
    "tar = [[1], [1], [0], [1], [0]]\n",
    "output = torch.tensor([out], dtype=torch.float32)\n",
    "targets = torch.tensor([tar], dtype=torch.float32)\n",
    "loss_function(output, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1024]),\n",
       " tensor(0.8201, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " tensor(0.3237))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50Seg(in_channels=3, out_channels=1)\n",
    "x = torch.randn(16, 3, 192, 192)  # input image\n",
    "targets = torch.randn(16, 1024)  # input image\n",
    "\n",
    "# Normalize x to be between 0 and 1\n",
    "x = torch.sigmoid(x)\n",
    "\n",
    "# Clip targets to 0 and 1\n",
    "targets = torch.clamp(targets, 0, 1)\n",
    "\n",
    "\n",
    "output = model(x, targets)\n",
    "output['logits'].shape, output['loss'], output['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8094)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected value for the BCE loss if there is 1024 elements\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "y_true = torch.randn(1024, 1024) \n",
    "y_pred = torch.randn(1024, 1024) \n",
    "\n",
    "loss = loss_function(y_pred, y_true)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.1933)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected value for the BCE loss if there is 1024 elements that were all correctly classified\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "y_pred = torch.randn(1024*16, 1024) \n",
    "\n",
    "loss = loss_function(y_pred, y_pred)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit on 1 batch of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/450 [00:00<03:06,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1121, Accuracy: 0.4919, Precision: 0.0982, F0.5: 0.1171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/450 [00:03<02:16,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0670, Accuracy: 0.6594, Precision: 0.2008, F0.5: 0.2367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 21/450 [00:06<02:12,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0315, Accuracy: 0.7725, Precision: 0.2935, F0.5: 0.3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 30/450 [00:09<02:22,  2.95it/s][E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "  7%|▋         | 30/450 [00:09<02:19,  3.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb Cell 24\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m f0point5 \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mf0point5\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Update weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = ResNet50Seg(in_channels=3, out_channels=1, class_one_weight=5)\n",
    "x = torch.randn(4, 3, 192, 192)  # input image\n",
    "# targets = random integers of shape (4, 1024) between 0 and 1\n",
    "targets = torch.randint(0, 2, (4, 1024)).float()\n",
    "# Make 80% of the targets to be 0\n",
    "targets[:, 0:819] = 0\n",
    "\n",
    "\n",
    "# Normalize x to be between 0 and 1\n",
    "x = torch.sigmoid(x)\n",
    "\n",
    "# Clip targets to 0 and 1\n",
    "targets = torch.clamp(targets, 0, 1)\n",
    "\n",
    "# optimizer \n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "n_iters = 450\n",
    "for i in tqdm(range(n_iters)):\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x, targets)\n",
    "    loss = output['loss']\n",
    "    precision = output['precision']\n",
    "    accuracy = output['accuracy']\n",
    "    f0point5 = output['f0point5']\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}, Precision: {precision.item():.4f}, F0.5: {f0point5.item():.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import avg_pool2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', device='cpu', cache_refresh_interval=None, cache_n_images=64):\n",
    "        \n",
    "        assert mode in ['train', 'test'], \"mode must be either 'train' or 'test'\"\n",
    "        \n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.cache_refresh_interval = cache_refresh_interval\n",
    "        self.cache_n_images = cache_n_images\n",
    "        self._load_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.volume_images)\n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Get the volume paths\n",
    "        self.volume_images = os.listdir(os.path.join(self.root, self.mode, 'volume'))\n",
    "        self.volume_images = [os.path.join(self.root, self.mode, 'volume', image) for image in self.volume_images]\n",
    "        \n",
    "        # Get label paths\n",
    "        self.label_images = os.listdir(os.path.join(self.root, self.mode, 'label'))\n",
    "        self.label_images = [os.path.join(self.root, self.mode, 'label', image) for image in self.label_images]\n",
    "        \n",
    "        # take self.cache_n_images random images from the dataset. They cannot be repeated\n",
    "        if self.cache_n_images is not None and self.cache_n_images < len(self.volume_images):\n",
    "            self.volume_images = np.random.choice(self.volume_images, size=self.cache_n_images, replace=False)\n",
    "            self.label_images = np.random.choice(self.label_images, size=self.cache_n_images, replace=False)\n",
    "        \n",
    "        # Load the data into memory\n",
    "        self.cached_data = []\n",
    "        for volume_image, label_image in zip(self.volume_images, self.label_images):\n",
    "            volume = np.load(volume_image)\n",
    "            label = np.load(label_image)\n",
    "            \n",
    "            # convert to torch tensors\n",
    "            volume = torch.tensor(volume, dtype=torch.float32)\n",
    "            label = torch.tensor(label, dtype=torch.float32)\n",
    "            \n",
    "            self.cached_data.append({\n",
    "                \"image\": volume,\n",
    "                \"targets\": label\n",
    "            }) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cached_data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.cached_data[item]\n",
    "    \n",
    "    def refresh_cache(self):\n",
    "        self._load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset and dataloader\n",
    "root = \"../../datasets/dataset-1/\"\n",
    "\n",
    "# Mac m1 device\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "dataset = ImageSegmentationDataset(root=root, mode='train', device=device, cache_refresh_interval=160, cache_n_images=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 384, 384, 16]), torch.Size([16, 384, 384, 1]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one batch\n",
    "batch = next(iter(dataloader))\n",
    "batch['image'].shape, batch['targets'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 100 iterations: 1.8975 seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's check how fast the dataloader is\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    batch = next(iter(dataloader))\n",
    "    # Refresh the cache \n",
    "    if i % dataset.cache_refresh_interval == 0:\n",
    "        dataset.refresh_cache()\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken for 100 iterations: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.1 ms ± 3.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.load(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/volume/0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 768, 64)\n",
      "(192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/volume/0.npy\")\n",
    "print(x.shape)\n",
    "x = x[::4, ::4, :3]\n",
    "print(x.shape)\n",
    "np.save(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/tmp/x.npy\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 µs ± 2.63 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.load(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/tmp/x.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 µs ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.load(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/label/0.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: make it's best to generate dataset-1 by taking 4x4 avg_2d while creating the dataset. This will make the dataloader much much faster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'img_width': 384, 'in_channels': 16, 'out_channels': 1, 'init_features': 64, 'class_one_weight': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 64, 'num_steps': 100000, 'log_freq': 100, 'eval_steps': 100, 'dataset_dir': '../../datasets/dataset-1', 'device': 'cuda', 'cache_refresh_interval': 64, 'cache_n_images': 64, 'wandb': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Args loaded (dict form): {'img_width': 384, 'in_channels': 16, 'out_channels': 1, 'init_features': 64, 'class_one_weight': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 64, 'num_steps': 100000, 'log_freq': 100, 'eval_steps': 100, 'dataset_dir': '../../datasets/dataset-1', 'device': 'cuda', 'cache_refresh_interval': 64, 'cache_n_images': 64, 'wandb': False}\n",
      "[LOG] img_width: 384\n",
      "[LOG] in_channels: 16\n",
      "[LOG] out_channels: 1\n",
      "[LOG] init_features: 64\n",
      "[LOG] class_one_weight: 1\n",
      "[LOG] seed: 42\n",
      "[LOG] learning_rate: 0.0001\n",
      "[LOG] batch_size: 64\n",
      "[LOG] num_steps: 100000\n",
      "[LOG] log_freq: 100\n",
      "[LOG] eval_steps: 100\n",
      "[LOG] dataset_dir: ../../datasets/dataset-1\n",
      "[LOG] device: cuda\n",
      "[LOG] cache_refresh_interval: 64\n",
      "[LOG] cache_n_images: 64\n",
      "[LOG] wandb: False\n",
      "Creating a model...\n",
      "Model created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from dataloader import ImageSegmentationDataset\n",
    "from model import UNet\n",
    "from eval import evaluate\n",
    "\n",
    "\n",
    "\n",
    "print(f\"[LOG] Args loaded (dict form): {args}\")\n",
    "for key, value in args.items():\n",
    "    print(f\"[LOG] {key}: {value}\")\n",
    "\n",
    "# Set the device\n",
    "if args['device'] == 'cuda':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageSegmentationDataset(root=args['dataset_dir'], mode='train', device=device, cache_refresh_interval=args['cache_refresh_interval'], cache_n_images=args['cache_n_images'])\n",
    "test_dataset = ImageSegmentationDataset(root=args['dataset_dir'], mode='test', device=device, cache_refresh_interval=args['cache_refresh_interval'], cache_n_images=args['cache_n_images'])\n",
    "\n",
    "train_dataloader =  DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "test_dataloader =  DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "# Create a model\n",
    "print(\"Creating a model...\")\n",
    "model = UNet(img_width=args['img_width'], in_channels=args['in_channels'], out_channels=args['out_channels'], init_features=args['init_features'], class_one_weight=args['class_one_weight'])\n",
    "print(\"Model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up wandb\n",
    "if args['wandb']:\n",
    "    wandb.login(key=\"YOUR_WANDB_API_KEY\")\n",
    "    wandb.init(project='vesuvius-challenge', config=args)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "# Initialize the metrics dictionary\n",
    "metrics = {\n",
    "    \"loss\": [],\n",
    "    \"logits\": [],\n",
    "    \"predictions\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"f0point5\": []\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame(columns=[\"step\"] + list(metrics.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384, 384, 16]), torch.Size([64, 384, 384, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "image, labels = batch['image'], batch['targets']\n",
    "image.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 384, 384])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.permute(0, 3, 1, 2).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in_channels': 16,\n",
       " 'out_channels': 1,\n",
       " 'init_features': 8,\n",
       " 'class_one_weight': 0.1,\n",
       " 'seed': 42,\n",
       " 'learning_rate': 0.0001,\n",
       " 'batch_size': 16,\n",
       " 'num_steps': 20000000,\n",
       " 'log_freq': 128,\n",
       " 'eval_steps': 64,\n",
       " 'dataset_dir': '/home/viktor/Documents/kaggle/vesuvius-challenge/datasets/dataset-1',\n",
       " 'device': 'cpu',\n",
       " 'cache_refresh_interval': 128,\n",
       " 'cache_n_images': 256,\n",
       " 'wandb': False}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {'in_channels': 16, 'out_channels': 1, 'init_features': 8, 'class_one_weight': 0.1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 16, 'num_steps': 20000000, 'log_freq': 128, 'eval_steps': 64, 'dataset_dir': '/home/viktor/Documents/kaggle/vesuvius-challenge/datasets/dataset-1', 'device': 'cpu', 'cache_refresh_interval': 128, 'cache_n_images': 256, 'wandb': False}\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from dataloaders import dataloader_ds1\n",
    "from models import unet\n",
    "from eval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Args loaded (dict form): {'in_channels': 16, 'out_channels': 1, 'init_features': 8, 'class_one_weight': 0.1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 16, 'num_steps': 20000000, 'log_freq': 128, 'eval_steps': 64, 'dataset_dir': '/home/viktor/Documents/kaggle/vesuvius-challenge/datasets/dataset-1', 'device': 'cpu', 'cache_refresh_interval': 128, 'cache_n_images': 256, 'wandb': False}\n",
      "[LOG] in_channels: 16\n",
      "[LOG] out_channels: 1\n",
      "[LOG] init_features: 8\n",
      "[LOG] class_one_weight: 0.1\n",
      "[LOG] seed: 42\n",
      "[LOG] learning_rate: 0.0001\n",
      "[LOG] batch_size: 16\n",
      "[LOG] num_steps: 20000000\n",
      "[LOG] log_freq: 128\n",
      "[LOG] eval_steps: 64\n",
      "[LOG] dataset_dir: /home/viktor/Documents/kaggle/vesuvius-challenge/datasets/dataset-1\n",
      "[LOG] device: cpu\n",
      "[LOG] cache_refresh_interval: 128\n",
      "[LOG] cache_n_images: 256\n",
      "[LOG] wandb: False\n",
      "Creating a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/viktor/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created.\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000000 [00:07<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[LOG] Args loaded (dict form): {args}\")\n",
    "for key, value in args.items():\n",
    "    print(f\"[LOG] {key}: {value}\")\n",
    "\n",
    "# Set the device\n",
    "if args['device'] == 'cuda' or args['device'] == 'gpu':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# Load datasets\n",
    "train_dataset = dataloader_ds1.ImageSegmentationDataset(root=args['dataset_dir'], mode='train', device=device, cache_refresh_interval=args['cache_refresh_interval'], cache_n_images=args['cache_n_images'])\n",
    "test_dataset = dataloader_ds1.ImageSegmentationDataset(root=args['dataset_dir'], mode='test', device=device, cache_refresh_interval=args['cache_refresh_interval'], cache_n_images=args['cache_n_images'])\n",
    "\n",
    "train_dataloader =  DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "test_dataloader =  DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "# Create a model\n",
    "print(\"Creating a model...\")\n",
    "model = unet.UNet(in_channels=args['in_channels'], out_channels=args['out_channels'], init_features=args['init_features'], class_one_weight=args['class_one_weight'])\n",
    "# send to device\n",
    "model.to(device)\n",
    "print(\"Model created.\")\n",
    "\n",
    "\n",
    "# Set up wandb\n",
    "if args['wandb']:\n",
    "    wandb.login(key=\"YOUR_WANDB_API_KEY\")\n",
    "    wandb.init(project='vesuvius-challenge', config=args)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "# Initialize the metrics dictionary\n",
    "metrics = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"f0point5\": []\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame(columns=[\"step\"] + list(metrics.keys()))\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for step in tqdm(range(args['num_steps'])):\n",
    "    # refresh the cache \n",
    "    if step % args['cache_refresh_interval'] == 0:\n",
    "        train_dataset.reset()\n",
    "        test_dataset.reset()\n",
    "        \n",
    "        # Re-initialize the dataloaders\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "    \n",
    "    \n",
    "    # Forward pass and compute the loss\n",
    "    batch = next(iter(train_dataloader))\n",
    "    images, labels = batch['image'], batch['targets']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder1): Sequential(\n",
       "    (enc1conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc1norm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc1relu1): ReLU(inplace=True)\n",
       "    (enc1conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc1norm2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc1relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder2): Sequential(\n",
       "    (enc2conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc2norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc2relu1): ReLU(inplace=True)\n",
       "    (enc2conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc2norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc2relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder3): Sequential(\n",
       "    (enc3conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc3norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc3relu1): ReLU(inplace=True)\n",
       "    (enc3conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc3norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc3relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder4): Sequential(\n",
       "    (enc4conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc4norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc4relu1): ReLU(inplace=True)\n",
       "    (enc4conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc4norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc4relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): Sequential(\n",
       "    (bottleneckconv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bottlenecknorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bottleneckrelu1): ReLU(inplace=True)\n",
       "    (bottleneckconv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bottlenecknorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bottleneckrelu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder4): Sequential(\n",
       "    (dec4conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec4norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec4relu1): ReLU(inplace=True)\n",
       "    (dec4conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec4norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec4relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder3): Sequential(\n",
       "    (dec3conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec3norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec3relu1): ReLU(inplace=True)\n",
       "    (dec3conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec3norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec3relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv2): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder2): Sequential(\n",
       "    (dec2conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec2norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec2relu1): ReLU(inplace=True)\n",
       "    (dec2conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec2norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec2relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv1): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder1): Sequential(\n",
       "    (dec1conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec1norm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec1relu1): ReLU(inplace=True)\n",
       "    (dec1conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec1norm2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec1relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# set weight of each Conv2d layer to have 0 mean and stddev of 1\n",
    "for layer in model.unet.modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        print(layer)\n",
    "        nn.init.normal_(layer.weight, mean=0, std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = model(images.permute(0, 3, 1, 2), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9433, 0.8452, 0.9371,  ..., 0.9721, 0.8856, 0.9555],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = out['loss']\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the weights in a single list\n",
    "weights = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the gradients\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model.unet is a unet model\n",
    "grads = []\n",
    "for name, param in model.unet.named_parameters():\n",
    "    if param.requires_grad and \"weight\" in name:\n",
    "        grads.append(param.grad.flatten().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = np.concatenate(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = np.abs(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log10(grads), bins=1000);\n",
    "# plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
