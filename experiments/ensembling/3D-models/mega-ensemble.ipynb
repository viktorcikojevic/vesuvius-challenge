{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## summary\n","\n","* 2.5d segmentation\n","    *  segmentation_models_pytorch \n","    *  Unet\n","* use only 6 slices\n","* slide inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:20.319372Z","iopub.status.busy":"2023-05-24T05:23:20.319033Z","iopub.status.idle":"2023-05-24T05:23:25.593778Z","shell.execute_reply":"2023-05-24T05:23:25.59234Z","shell.execute_reply.started":"2023-05-24T05:23:20.319327Z"},"trusted":true},"outputs":[],"source":["\n","from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, log_loss\n","import pickle\n","from torch.utils.data import DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","import warnings\n","import sys\n","import pandas as pd\n","import os\n","import gc\n","import sys\n","import math\n","import time\n","import random\n","import shutil\n","from pathlib import Path\n","from contextlib import contextmanager\n","from collections import defaultdict, Counter\n","import cv2\n","\n","import scipy as sp\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","from functools import partial\n","\n","import argparse\n","import importlib\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam, SGD, AdamW\n","import torch.nn.functional as F\n","\n","\n","import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:25.597597Z","iopub.status.busy":"2023-05-24T05:23:25.596834Z","iopub.status.idle":"2023-05-24T05:23:28.181759Z","shell.execute_reply":"2023-05-24T05:23:28.180675Z","shell.execute_reply.started":"2023-05-24T05:23:25.597551Z"},"trusted":true},"outputs":[],"source":["sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n","sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n","sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n","sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n","\n","\n","import segmentation_models_pytorch as smp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:28.18771Z","iopub.status.busy":"2023-05-24T05:23:28.185496Z","iopub.status.idle":"2023-05-24T05:23:29.055313Z","shell.execute_reply":"2023-05-24T05:23:29.054269Z","shell.execute_reply.started":"2023-05-24T05:23:28.187676Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from torch.utils.data import DataLoader, Dataset\n","import cv2\n","import torch\n","import os\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from albumentations import ImageOnlyTransform"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.05907Z","iopub.status.busy":"2023-05-24T05:23:29.058218Z","iopub.status.idle":"2023-05-24T05:23:29.072045Z","shell.execute_reply":"2023-05-24T05:23:29.071024Z","shell.execute_reply.started":"2023-05-24T05:23:29.059028Z"},"trusted":true},"outputs":[],"source":["sys.path.append(\"/kaggle/input/resnet3d\")\n","\n","from resnet3d import generate_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# fragment, model, threshold, image_size"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.07576Z","iopub.status.busy":"2023-05-24T05:23:29.074962Z","iopub.status.idle":"2023-05-24T05:23:29.090833Z","shell.execute_reply":"2023-05-24T05:23:29.089813Z","shell.execute_reply.started":"2023-05-24T05:23:29.07572Z"},"trusted":true},"outputs":[],"source":["import os\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","class CFG:\n","    \n","    in_chans = 16 # 65\n","    # ============== training cfg =============\n","    size = 256\n","    tile_size = 256\n","    stride = tile_size // 2\n","\n","    batch_size = 24\n","    \n","\n","    valid_aug_list = [\n","        ToTensorV2(transpose_mask=True),\n","    ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.106589Z","iopub.status.busy":"2023-05-24T05:23:29.106116Z","iopub.status.idle":"2023-05-24T05:23:29.175766Z","shell.execute_reply":"2023-05-24T05:23:29.174622Z","shell.execute_reply.started":"2023-05-24T05:23:29.106512Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## helper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.179781Z","iopub.status.busy":"2023-05-24T05:23:29.178967Z","iopub.status.idle":"2023-05-24T05:23:29.188417Z","shell.execute_reply":"2023-05-24T05:23:29.186752Z","shell.execute_reply.started":"2023-05-24T05:23:29.179736Z"},"trusted":true},"outputs":[],"source":["# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n","def rle(img):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    pixels = img.flatten()\n","    # pixels = (pixels >= thr).astype(int)\n","    \n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.192552Z","iopub.status.busy":"2023-05-24T05:23:29.189727Z","iopub.status.idle":"2023-05-24T05:23:29.202852Z","shell.execute_reply":"2023-05-24T05:23:29.201723Z","shell.execute_reply.started":"2023-05-24T05:23:29.19252Z"},"trusted":true},"outputs":[],"source":["def read_image(fragment_id):\n","    images = []\n","\n","    # idxs = range(65)\n","    # mid = 65 // 2\n","    start = 20 # mid - CFG.in_chans // 2\n","    end = 40 # mid + CFG.in_chans // 2\n","    idxs = range(start, end)\n","\n","    for i in tqdm(idxs):\n","        \n","        image = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n","        \n","        if np.abs(np.max(image) - np.min(image)) > 1e-3:\n","            image = (image - np.min(image)) / (np.max(image) - np.min(image))\n","        \n","\n","        pad0 = (CFG.tile_size - image.shape[0] % CFG.tile_size)\n","        pad1 = (CFG.tile_size - image.shape[1] % CFG.tile_size)\n","\n","        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","        images.append(image)\n","    images = np.stack(images, axis=2)\n","    \n","    return images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.20837Z","iopub.status.busy":"2023-05-24T05:23:29.20809Z","iopub.status.idle":"2023-05-24T05:23:29.218188Z","shell.execute_reply":"2023-05-24T05:23:29.217123Z","shell.execute_reply.started":"2023-05-24T05:23:29.208343Z"},"trusted":true},"outputs":[],"source":["def get_transforms(data, cfg):\n","    if data == 'train':\n","        aug = A.Compose(cfg.train_aug_list)\n","    elif data == 'valid':\n","        aug = A.Compose(cfg.valid_aug_list)\n","\n","    # print(aug)\n","    return aug\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, images, cfg, labels=None, transform=None):\n","        self.images = images\n","        self.cfg = cfg\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        # return len(self.xyxys)\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # x1, y1, x2, y2 = self.xyxys[idx]\n","        image = self.images[idx]\n","        data = self.transform(image=image)\n","        image = data['image']\n","        # image = (image - 0.45)/0.225\n","        \n","        \n","        \n","        return image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.221183Z","iopub.status.busy":"2023-05-24T05:23:29.22019Z","iopub.status.idle":"2023-05-24T05:23:29.231611Z","shell.execute_reply":"2023-05-24T05:23:29.230712Z","shell.execute_reply.started":"2023-05-24T05:23:29.221145Z"},"trusted":true},"outputs":[],"source":["def make_test_dataset(fragment_id):\n","    test_images = read_image(fragment_id)\n","    \n","    x1_list = list(range(0, test_images.shape[1]-CFG.tile_size+1, CFG.stride))\n","    y1_list = list(range(0, test_images.shape[0]-CFG.tile_size+1, CFG.stride))\n","    \n","    test_images_list = []\n","    xyxys = []\n","    for y1 in y1_list:\n","        for x1 in x1_list:\n","            y2 = y1 + CFG.tile_size\n","            x2 = x1 + CFG.tile_size\n","            \n","            test_images_list.append(test_images[y1:y2, x1:x2])\n","            xyxys.append((x1, y1, x2, y2))\n","    xyxys = np.stack(xyxys)\n","            \n","    test_dataset = CustomDataset(test_images_list, CFG, transform=get_transforms(data='valid', cfg=CFG))\n","    \n","    test_loader = DataLoader(test_dataset,\n","                          batch_size=CFG.batch_size,\n","                          shuffle=False,\n","                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n","    \n","    return test_loader, xyxys"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.233958Z","iopub.status.busy":"2023-05-24T05:23:29.233167Z","iopub.status.idle":"2023-05-24T05:23:29.250225Z","shell.execute_reply":"2023-05-24T05:23:29.249264Z","shell.execute_reply.started":"2023-05-24T05:23:29.233917Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, encoder_dims, upscale):\n","        super().__init__()\n","        self.convs = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),\n","                nn.BatchNorm2d(encoder_dims[i-1]),\n","                nn.ReLU(inplace=True)\n","            ) for i in range(1, len(encoder_dims))])\n","\n","        self.logit = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)\n","        self.up = nn.Upsample(scale_factor=upscale, mode=\"bilinear\")\n","\n","    def forward(self, feature_maps):\n","        for i in range(len(feature_maps)-1, 0, -1):\n","            f_up = F.interpolate(feature_maps[i], scale_factor=2, mode=\"bilinear\")\n","            f = torch.cat([feature_maps[i-1], f_up], dim=1)\n","            f_down = self.convs[i-1](f)\n","            feature_maps[i-1] = f_down\n","\n","        x = self.logit(feature_maps[0])\n","        mask = self.up(x)\n","        return mask\n","\n","\n","class SegModel(nn.Module):\n","    def __init__(self, model_depth):\n","        super().__init__()\n","        self.encoder = generate_model(model_depth=model_depth, n_input_channels=1)\n","        self.decoder = Decoder(encoder_dims=[64, 128, 256, 512], upscale=4)\n","        \n","    def forward(self, x):\n","        feat_maps = self.encoder(x)\n","        feat_maps_pooled = [torch.mean(f, dim=2) for f in feat_maps]\n","        pred_mask = self.decoder(feat_maps_pooled)\n","        return pred_mask\n","    \n","    def load_pretrained_weights(self, state_dict):\n","        # Convert 3 channel weights to single channel\n","        # ref - https://timm.fast.ai/models#Case-1:-When-the-number-of-input-channels-is-1\n","        conv1_weight = state_dict['conv1.weight']\n","        state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n","        print(self.encoder.load_state_dict(state_dict, strict=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.25253Z","iopub.status.busy":"2023-05-24T05:23:29.251855Z","iopub.status.idle":"2023-05-24T05:23:29.264589Z","shell.execute_reply":"2023-05-24T05:23:29.263546Z","shell.execute_reply.started":"2023-05-24T05:23:29.252491Z"},"trusted":true},"outputs":[],"source":["# class EnsembleModel:\n","#     def __init__(self):\n","#         self.models = []\n","#         self.thresholds = []\n","#         self.image_sizes = []\n","\n","#     def __call__(self, x):\n","\n","#         img_wh = x.shape[1]\n","        \n","#         outputs = []\n","#         for model, threshold in zip(self.models, self.thresholds):\n","#             for img_size in self.image_sizes:\n","#                 if img_wh == img_size:\n","#                     out = (torch.sigmoid(model(x)).to('cpu').numpy()>=threshold).astype(np.float32)\n","#                     outputs.append(out)\n","        \n","#         avg_preds = np.mean(outputs, axis=0)\n","        \n","#         return avg_preds\n","\n","#     def add_model(self, model, threshold, image_size):\n","#         self.models.append(model)\n","#         self.thresholds.append(threshold)\n","#         self.image_sizes.append(image_size)\n","\n","# def build_ensemble_model():\n","#     model = EnsembleModel()\n","    \n","#     # \"bce-dice-models/fold-1/depth-18/image-256/ckpts/ # resnet34_3d_seg_best_0.900.pt\n","#     folds = [\"fold-1\", \"fold-2\", \"fold-3\"]\n","#     depths = [\"depth-18\", \"depth-34\"]\n","#     images = [\"image-256\", \"image-512\"]\n","#     for fold in folds:\n","#         for depth in depths:\n","#             for image in images:\n","#                 dir_path = f\"bce-dice-models/{fold}/{depth}/{image}/ckpts/\"\n","#                 # get .pt model from the dir_path\n","#                 model_path = dir_path + [file for file in os.listdir(dir_path) if file.endswith(\".pt\")][0]\n","#                 # get the model threshold: it is of the format resnet34_3d_seg_best_0.900.pt\n","#                 model_threshold = float(model_path.split(\"_\")[-1].split(\".\")[0])\n","#                 model_path = dir_path + model_path\n","                \n","#                 model.add_model(SegModel().to(CFG.device).eval(), 0.5)\n","    \n","    \n","#     thresholds = np.array([0.45, 0.4, 0.5]) + 0.2\n","    \n","#     for fold in range(len(model_paths)):\n","#         # _model = build_model(CFG, weight=None)\n","#         _model = SegModel()\n","#         model_path = model_paths[fold]\n","#         _model.load_state_dict(torch.load(model_path))\n","#         _model.to(device)\n","#         _model.eval()\n","#         threshold = thresholds[fold]\n","        \n","#         model.add_model(_model, threshold)\n","    \n","#     return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.281059Z","iopub.status.busy":"2023-05-24T05:23:29.280511Z","iopub.status.idle":"2023-05-24T05:23:29.294Z","shell.execute_reply":"2023-05-24T05:23:29.292975Z","shell.execute_reply.started":"2023-05-24T05:23:29.281022Z"},"trusted":true},"outputs":[],"source":["if mode == 'test':\n","    fragment_ids = sorted(os.listdir(CFG.comp_dataset_path + mode))\n","else:\n","    fragment_ids = [3]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:29.296343Z","iopub.status.busy":"2023-05-24T05:23:29.295752Z","iopub.status.idle":"2023-05-24T05:23:38.838871Z","shell.execute_reply":"2023-05-24T05:23:38.837778Z","shell.execute_reply.started":"2023-05-24T05:23:29.2963Z"},"trusted":true},"outputs":[],"source":["# model = build_ensemble_model()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["I have the following structure of models.\n","I want to \n","\n","```bash\n","├── fold-1\n","│   ├── depth-18\n","│   │   ├── image-256\n","│   │   │   ├── ckpts\n","│   │   │   │   └── resnet34_3d_seg_best_0.900.pt\n","│   │   │   ├── vesuvius-challenge-3d-resnet-training-step-2-with-augm.ipynb\n","│   │   └── image-512\n","│   │       ├── ckpts\n","│   │       │   └── resnet34_3d_seg_best_0.900.pt\n","│   └── depth-34\n","│       ├── image-256\n","│       │   ├── ckpts\n","│       │   │   └── resnet34_3d_seg_best_0.800.pt\n","│       └── image-512\n","│           ├── ckpts\n","│           │   └── resnet34_3d_seg_best_0.800.pt\n","├── fold-2\n","│   ├── depth-18\n","│   │   ├── image-256\n","│   │   │   ├── ckpts\n","│   │   │   │   └── resnet34_3d_seg_best_0.200.pt\n","│   │   └── image-512\n","│   │       ├── ckpts\n","│   │       │   └── resnet34_3d_seg_best_0.200.pt\n","│   └── depth-34\n","│       ├── image-256\n","│       │   ├── ckpts\n","│       │   │   └── resnet34_3d_seg_best_0.200.pt\n","│       └── image-512\n","│           ├── ckpts\n","│           │   └── resnet34_3d_seg_best_0.200.pt\n","├── fold-3\n","│   ├── depth-18\n","│   │   ├── image-256\n","│   │   │   ├── ckpts\n","│   │   │   │   └── resnet34_3d_seg_best_0.850.pt\n","│   │   └── image-512\n","│   │       ├── ckpts\n","│   │       │   └── resnet34_3d_seg_best_0.650.pt\n","│   └── depth-34\n","│       ├── image-256\n","│       │   ├── ckpts\n","│       │   │   └── resnet34_3d_seg_best_0.800.pt\n","│       └── image-512\n","│           ├── ckpts\n","│           │   └── resnet34_3d_seg_best_0.800.pt\n","└── prepare-experiment.sh\n","```"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## main"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T05:23:38.840762Z","iopub.status.busy":"2023-05-24T05:23:38.840397Z","iopub.status.idle":"2023-05-24T05:26:44.235693Z","shell.execute_reply":"2023-05-24T05:26:44.233964Z","shell.execute_reply.started":"2023-05-24T05:23:38.84072Z"},"trusted":true},"outputs":[],"source":["results = []\n","\n","folds = [\"fold-1\", \"fold-2\", \"fold-3\"]\n","depths = [\"depth-18\", \"depth-34\"]\n","image_sizes = [\"image-256\", \"image-512\"]\n","\n","\n","for fragment_id in fragment_ids:\n","    \n","    CFG.size = 256\n","    CFG.tile_size = 256\n","    CFG.stride = 256\n","    test_loader_256, xyxys_256 = make_test_dataset(fragment_id)\n","    \n","    CFG.size = 512\n","    CFG.tile_size = 512\n","    CFG.stride = 512\n","    test_loader_512, xyxys_512 = make_test_dataset(fragment_id)\n","    \n","    final_mask_pred = np.zeros_like(binary_mask)\n","    \n","    for fold in folds:\n","        for depth in depths:\n","            for image_size in image_sizes:\n","                dir_path = f\"bce-dice-models/{fold}/{depth}/{image_size}/ckpts/\"\n","                # get .pt model from the dir_path\n","                model_path = dir_path + [file for file in os.listdir(dir_path) if file.endswith(\".pt\")][0]\n","                # get the model threshold: it is of the format resnet34_3d_seg_best_0.900.pt\n","                model_threshold = float(model_path.split(\"_\")[-1].split(\".\")[0])\n","                model_path = dir_path + model_path\n","                \n","                \n","                image_wh = int(image_size.split(\"-\")[-1])\n","                CFG.size = image_wh\n","                CFG.tile_size = image_wh\n","                CFG.stride = image_wh\n","                \n","                model_depth = int(depth.split(\"-\")[-1])\n","                model = SegModel(model_depth)\n","                model.load_state_dict(torch.load(model_path))\n","                model = model.to(CFG.device).eval()\n","                \n","    \n","\n","                binary_mask = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/mask.png\", 0)\n","                binary_mask = (binary_mask / 255).astype(int)\n","                \n","                ori_h = binary_mask.shape[0]\n","                ori_w = binary_mask.shape[1]\n","                # mask = mask / 255\n","                \n","                \n","                pad0 = (CFG.tile_size - binary_mask.shape[0] % CFG.tile_size)\n","                pad1 = (CFG.tile_size - binary_mask.shape[1] % CFG.tile_size)\n","                \n","                binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","                mask_pred = np.zeros(binary_mask.shape)\n","                mask_count = np.zeros(binary_mask.shape)\n","\n","                if image_wh == 256:\n","                    test_loader = test_loader_256\n","                    xyxys = xyxys_256\n","                else:\n","                    test_loader = test_loader_512\n","                    xyxys = xyxys_512\n","                    \n","                for step, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n","                    images = images.to(device)\n","                    images = images.unsqueeze(1)\n","\n","                    \n","                    batch_size = images.size(0)\n","\n","                    with torch.no_grad():\n","                        y_preds = model(images)\n","                        y_preds = torch.sigmoid(y_preds)\n","\n","                    start_idx = step*CFG.batch_size\n","                    end_idx = start_idx + batch_size\n","                    for i, (x1, y1, x2, y2) in enumerate(xyxys[start_idx:end_idx]):\n","                        mask_pred[y1:y2, x1:x2] += y_preds[i].squeeze(0)\n","                        mask_count[y1:y2, x1:x2] += np.ones((CFG.tile_size, CFG.tile_size))\n","        \n","                plt.imshow(mask_count)\n","                plt.show()\n","    \n","                print(f'mask_count_min: {mask_count.min()}')\n","                mask_pred /= mask_count\n","    \n","                mask_pred = mask_pred[:ori_h, :ori_w]\n","                binary_mask = binary_mask[:ori_h, :ori_w]\n","                \n","                \n","                mask_pred = (mask_pred >= model_threshold).astype(int)\n","                mask_pred = mask_pred * binary_mask\n","                \n","                final_mask_pred.append(mask_pred)\n","                \n","             \n","    final_mask_pred = np.mean(final_mask_pred, axis=0)\n","    # apply final_threshold\n","    final_threshold = 8./12\n","    final_mask_pred = (final_mask_pred > final_threshold).astype(int)\n","       \n","    \n","    plt.imshow(final_mask_pred)\n","    plt.show()\n","    \n","    inklabels_rle = rle(final_mask_pred)\n","    \n","    results.append((fragment_id, inklabels_rle))\n","    \n","\n","    del final_mask_pred, mask_count\n","    del test_loader\n","    \n","    gc.collect()\n","    torch.cuda.empty_cache()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T05:26:44.237489Z","iopub.status.idle":"2023-05-24T05:26:44.238372Z","shell.execute_reply":"2023-05-24T05:26:44.238113Z","shell.execute_reply.started":"2023-05-24T05:26:44.238084Z"},"trusted":true},"outputs":[],"source":["sub = pd.DataFrame(results, columns=['Id', 'Predicted'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T05:26:44.239975Z","iopub.status.idle":"2023-05-24T05:26:44.240835Z","shell.execute_reply":"2023-05-24T05:26:44.240575Z","shell.execute_reply.started":"2023-05-24T05:26:44.240549Z"},"trusted":true},"outputs":[],"source":["sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T05:26:44.24232Z","iopub.status.idle":"2023-05-24T05:26:44.243169Z","shell.execute_reply":"2023-05-24T05:26:44.242933Z","shell.execute_reply.started":"2023-05-24T05:26:44.242906Z"},"trusted":true},"outputs":[],"source":["sample_sub = pd.read_csv(CFG.comp_dataset_path + 'sample_submission.csv')\n","sample_sub = pd.merge(sample_sub[['Id']], sub, on='Id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T05:26:44.244629Z","iopub.status.idle":"2023-05-24T05:26:44.24547Z","shell.execute_reply":"2023-05-24T05:26:44.245219Z","shell.execute_reply.started":"2023-05-24T05:26:44.245193Z"},"trusted":true},"outputs":[],"source":["sample_sub"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T05:26:44.246975Z","iopub.status.idle":"2023-05-24T05:26:44.247823Z","shell.execute_reply":"2023-05-24T05:26:44.247569Z","shell.execute_reply.started":"2023-05-24T05:26:44.247542Z"},"trusted":true},"outputs":[],"source":["sample_sub.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
