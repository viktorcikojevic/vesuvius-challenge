{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Takes as input the (768, 768, 64) images from `dataset-1`.\n",
    "- It takes 4^3 average across the 2D, and average across the depth\n",
    "- This results in a (192, 192, 16) volume.\n",
    "- I have a image that is of shape (1, 192, 192, 16). Divide the image and the labels into 32 patches: (1, 8*24, 8*24, 16) → (24, 24, 1024) → (576, 1024). \n",
    "- Pass the image into a transformer architecture with 6 layers, which will return (576, 1) tensor, which corresponds to probabilities of pixel occuring. You should also pass the “positional embeddings”, which should be of dimension 1024."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4^3 3D average \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def avg_3d(volume):\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    volume_tensor = torch.tensor(volume, dtype=torch.float32)\n",
    "\n",
    "    # Add batch and channel dimensions to the tensor\n",
    "    volume_tensor = volume_tensor.permute(0, 3, 1, 2)  # Reorder dimensions to (batch, channels, height, width)\n",
    "\n",
    "    # Create the 3D average pooling layer with the appropriate kernel size and stride values\n",
    "    avg_pool = nn.AvgPool3d(kernel_size=4, stride=4, padding=0)\n",
    "\n",
    "    # Apply the average pooling layer to the input tensor\n",
    "    with torch.no_grad():\n",
    "        filtered_volume_tensor = avg_pool(volume_tensor)\n",
    "\n",
    "    # Convert the output tensor back to a numpy array\n",
    "    filtered_volume = filtered_volume_tensor.permute(0, 2, 3, 1)  # Reorder dimensions back to (batch, height, width, channels)\n",
    "    \n",
    "    return filtered_volume\n",
    "\n",
    "\n",
    "# Example volume\n",
    "volume = np.random.rand(1, 768, 768, 64)\n",
    "volume_avgd = avg_3d(volume)\n",
    "# %time avg_3d(volume) # 85 ms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 192, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_avgd.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take random 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 192, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_avgd_new = volume_avgd.permute(3, 1, 2, 0)\n",
    "# take 3 random numbers between 0 and 16, without replacement\n",
    "random_numbers = np.random.choice(16, 3, replace=False)\n",
    "volume_avgd_new = volume_avgd_new[random_numbers, :, :, :]\n",
    "# Repermute\n",
    "volume_avgd_new = volume_avgd_new.permute(3, 1, 2, 0)\n",
    "volume_avgd_new.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def reshape_img(image):\n",
    "    # Calculate the size of each patch\n",
    "    patch_size = 24\n",
    "\n",
    "    B, H, W, C = image.shape\n",
    "    image = image.reshape(B, H // patch_size, patch_size, W // patch_size, patch_size, C) # (B, 8, 24, 8, 24, 16)\n",
    "    image = image.permute(0, 2, 4, 1, 3, 5) # (B, 24, 24, 8, 8, 16)\n",
    "    image = image.reshape(B, patch_size, patch_size, -1) # (B, 24, 24, 1024)\n",
    "    image = image.reshape(B, -1, 1024) # (B, 576, 1024)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Example image\n",
    "image = np.random.rand(2, 192, 192, 16)\n",
    "image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "\n",
    "image = reshape_img(image)\n",
    "\n",
    "print(image.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric competition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0point5_score(output, target):\n",
    "    # Flatten the output and target tensors\n",
    "    output = output.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    # Convert the output to binary values 0 and 1\n",
    "    output = (output > 0.5).float()\n",
    "    \n",
    "    # Calculate the precision and recall\n",
    "    tp = torch.sum(output * target)\n",
    "    fp = torch.sum(output * (1 - target))\n",
    "    fn = torch.sum((1 - output) * target)\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    \n",
    "    # Calculate the F0.5 score\n",
    "    beta = 0.5\n",
    "    f0point5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-7)\n",
    "    return f0point5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: ResNet50Seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet50Seg(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, class_one_weight=1):\n",
    "        super(ResNet50Seg, self).__init__()\n",
    "        \n",
    "        self.class_one_weight = class_one_weight\n",
    "        \n",
    "        # Load the pre-trained ResNet50 model\n",
    "        self.resnet50 = models.resnet50()\n",
    "        \n",
    "        # Replace the final layer to output 1024 channel instead of 1000\n",
    "        self.resnet50.fc = nn.Linear(2048, 1024)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.resnet50(x)\n",
    "        # Apply sigmoid\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        out = {}\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            precision = None\n",
    "            accuracy = None\n",
    "            f0point5 = None\n",
    "        else:\n",
    "            # Calculate the loss\n",
    "            \n",
    "            # Calculate class weights based on the imbalance\n",
    "            class_weights = torch.tensor([1.0, self.class_one_weight]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "            # Instantiate the loss function\n",
    "            loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "            \n",
    "            # Flatten the targets tensor\n",
    "            targets = targets.reshape(-1, 1024)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_function(x, targets)\n",
    "            \n",
    "            # Calculate the accuracy: number of correctly predicted pixel / total number of pixels\n",
    "            # Convert the predictions to binary values 0 and 1\n",
    "            predictions = (x > 0.5).float()\n",
    "            # Calculate the accuracy\n",
    "            accuracy = (predictions == targets).float().mean()\n",
    "            \n",
    "            # Calculate the precision\n",
    "            tp = torch.sum(predictions * targets)   \n",
    "            fp = torch.sum(predictions * (1 - targets))\n",
    "            fn = torch.sum((1 - predictions) * targets)\n",
    "            precision = tp / (tp + fp + 1e-7)\n",
    "            \n",
    "            # Calculate F0.5 score\n",
    "            f0point5 = f0point5_score(predictions, targets) \n",
    "        \n",
    "        out = {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": x,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"f0point5\": f0point5\n",
    "        }\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50Seg(in_channels=3, out_channels=1)\n",
    "x = torch.randn(1, 3, 192, 192)  # input image\n",
    "output = model(x)\n",
    "output['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.606208"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 1e6 parameters\n",
    "sum(p.numel() for p in model.parameters()) / 1e6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode: UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 192, 192])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=32, out_channels=1, init_features=64, pretrained=False)\n",
    "\n",
    "# example image\n",
    "image = torch.randn(1, 32, 192, 192)  # input image\n",
    "\n",
    "output = model(image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 256, 256]           9,216\n",
      "       BatchNorm2d-2         [-1, 32, 256, 256]              64\n",
      "              ReLU-3         [-1, 32, 256, 256]               0\n",
      "            Conv2d-4         [-1, 32, 256, 256]           9,216\n",
      "       BatchNorm2d-5         [-1, 32, 256, 256]              64\n",
      "              ReLU-6         [-1, 32, 256, 256]               0\n",
      "         MaxPool2d-7         [-1, 32, 128, 128]               0\n",
      "            Conv2d-8         [-1, 64, 128, 128]          18,432\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "           Conv2d-11         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-12         [-1, 64, 128, 128]             128\n",
      "             ReLU-13         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15          [-1, 128, 64, 64]          73,728\n",
      "      BatchNorm2d-16          [-1, 128, 64, 64]             256\n",
      "             ReLU-17          [-1, 128, 64, 64]               0\n",
      "           Conv2d-18          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-19          [-1, 128, 64, 64]             256\n",
      "             ReLU-20          [-1, 128, 64, 64]               0\n",
      "        MaxPool2d-21          [-1, 128, 32, 32]               0\n",
      "           Conv2d-22          [-1, 256, 32, 32]         294,912\n",
      "      BatchNorm2d-23          [-1, 256, 32, 32]             512\n",
      "             ReLU-24          [-1, 256, 32, 32]               0\n",
      "           Conv2d-25          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-26          [-1, 256, 32, 32]             512\n",
      "             ReLU-27          [-1, 256, 32, 32]               0\n",
      "        MaxPool2d-28          [-1, 256, 16, 16]               0\n",
      "           Conv2d-29          [-1, 512, 16, 16]       1,179,648\n",
      "      BatchNorm2d-30          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-31          [-1, 512, 16, 16]               0\n",
      "           Conv2d-32          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-34          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-35          [-1, 256, 32, 32]         524,544\n",
      "           Conv2d-36          [-1, 256, 32, 32]       1,179,648\n",
      "      BatchNorm2d-37          [-1, 256, 32, 32]             512\n",
      "             ReLU-38          [-1, 256, 32, 32]               0\n",
      "           Conv2d-39          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 32, 32]             512\n",
      "             ReLU-41          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-42          [-1, 128, 64, 64]         131,200\n",
      "           Conv2d-43          [-1, 128, 64, 64]         294,912\n",
      "      BatchNorm2d-44          [-1, 128, 64, 64]             256\n",
      "             ReLU-45          [-1, 128, 64, 64]               0\n",
      "           Conv2d-46          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 64, 64]             256\n",
      "             ReLU-48          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-49         [-1, 64, 128, 128]          32,832\n",
      "           Conv2d-50         [-1, 64, 128, 128]          73,728\n",
      "      BatchNorm2d-51         [-1, 64, 128, 128]             128\n",
      "             ReLU-52         [-1, 64, 128, 128]               0\n",
      "           Conv2d-53         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-54         [-1, 64, 128, 128]             128\n",
      "             ReLU-55         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-56         [-1, 32, 256, 256]           8,224\n",
      "           Conv2d-57         [-1, 32, 256, 256]          18,432\n",
      "      BatchNorm2d-58         [-1, 32, 256, 256]              64\n",
      "             ReLU-59         [-1, 32, 256, 256]               0\n",
      "           Conv2d-60         [-1, 32, 256, 256]           9,216\n",
      "      BatchNorm2d-61         [-1, 32, 256, 256]              64\n",
      "             ReLU-62         [-1, 32, 256, 256]               0\n",
      "           Conv2d-63          [-1, 1, 256, 256]              33\n",
      "================================================================\n",
      "Total params: 7,771,393\n",
      "Trainable params: 7,771,393\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 8.00\n",
      "Forward/backward pass size (MB): 404.00\n",
      "Params size (MB): 29.65\n",
      "Estimated Total Size (MB): 441.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (32, 256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 192, 192])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=False)\n",
    "\n",
    "# example image\n",
    "image = torch.randn(1, 192, 192, 16)  # input image\n",
    "\n",
    "# Make 3 dense layers that will make the number of channels from 16 to 3\n",
    "dense1 = nn.Linear(16, 16)\n",
    "dense2 = nn.Linear(16, 8)\n",
    "dense3 = nn.Linear(8, 3)\n",
    "\n",
    "image = image.reshape(-1, 16)  # Flatten the image\n",
    "image = dense1(image)  # Apply the first dense layer\n",
    "# relu\n",
    "image = torch.max(image, torch.zeros_like(image))\n",
    "image = dense2(image)  # Apply the second dense layer\n",
    "image = torch.max(image, torch.zeros_like(image))\n",
    "image = dense3(image)  # Apply the third dense layer\n",
    "image = torch.max(image, torch.zeros_like(image))\n",
    "\n",
    "# reshape to (1, 192, 192, 3)\n",
    "image = image.reshape(1, 192, 192, 3)\n",
    "# permute to (1, 3, 192, 192)\n",
    "image = image.permute(0, 3, 1, 2)\n",
    "\n",
    "output = model(image)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  5,  8, 10, 15, 18, 20, 26, 28, 29, 31, 32, 35, 42, 48, 60])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.random.choice(64, 16, replace=False)\n",
    "indices = np.sort(indices)\n",
    "indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Unet (tests for a script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_width=384, in_channels=16, out_channels=1, init_features=64, class_one_weight=1):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.img_width = img_width\n",
    "        self.class_one_weight = class_one_weight\n",
    "        \n",
    "        # Load the pre-trained UNet model\n",
    "        self.unet = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=in_channels, out_channels=out_channels, init_features=init_features, pretrained=False)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.unet(x)\n",
    "        # Convert the predictions to binary values 0 and 1\n",
    "        predictions = (x > 0.5).float()\n",
    "        \n",
    "        out = {}\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            precision = None\n",
    "            accuracy = None\n",
    "            f0point5 = None\n",
    "        else:\n",
    "            # Calculate the loss\n",
    "            \n",
    "            # Calculate class weights based on the imbalance\n",
    "            class_weights = torch.tensor([1.0, self.class_one_weight]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "            # Instantiate the loss function\n",
    "            loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "            \n",
    "            # Flatten the targets and x tensor\n",
    "            targets = targets.reshape(-1, int(self.img_width ** 2))\n",
    "            x = x.reshape(-1, int(self.img_width ** 2))\n",
    "            predictions_flat = predictions.reshape(-1, int(self.img_width ** 2))\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_function(x, targets)\n",
    "            \n",
    "            # Calculate the accuracy: number of correctly predicted pixel / total number of pixels\n",
    "            # Calculate the accuracy\n",
    "            accuracy = (predictions_flat == targets).float().mean()\n",
    "            \n",
    "            # Calculate the precision\n",
    "            tp = torch.sum(predictions_flat * targets)   \n",
    "            fp = torch.sum(predictions_flat * (1 - targets))\n",
    "            fn = torch.sum((1 - predictions_flat) * targets)\n",
    "            precision = tp / (tp + fp + 1e-7)\n",
    "            \n",
    "            # Calculate F0.5 score\n",
    "            f0point5 = f0point5_score(predictions_flat, targets) \n",
    "        \n",
    "        out = {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": x,\n",
    "            \"predictions\": predictions,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"f0point5\": f0point5\n",
    "        }\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': None,\n",
       " 'logits': tensor([[[[0.3822, 0.4928, 0.5304,  ..., 0.5390, 0.5003, 0.4663],\n",
       "           [0.3897, 0.4987, 0.3285,  ..., 0.4702, 0.4966, 0.4246],\n",
       "           [0.4077, 0.5033, 0.4310,  ..., 0.5758, 0.3895, 0.4993],\n",
       "           ...,\n",
       "           [0.3387, 0.4023, 0.4429,  ..., 0.4360, 0.3875, 0.4244],\n",
       "           [0.4495, 0.4083, 0.4170,  ..., 0.3509, 0.3790, 0.5283],\n",
       "           [0.2891, 0.3429, 0.4832,  ..., 0.3691, 0.3413, 0.3569]]]],\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " 'predictions': tensor([[[[0., 0., 1.,  ..., 1., 1., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 1., 0.,  ..., 1., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " 'accuracy': None,\n",
       " 'precision': None,\n",
       " 'f0point5': None}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(in_channels=16, out_channels=1)\n",
    "# example image\n",
    "image = torch.randn(1, 16, 192, 192)  # input image\n",
    "\n",
    "output = model(image)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.5615, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'logits': tensor([[0.5646, 0.5351, 0.4979,  ..., 0.5414, 0.4981, 0.4839]],\n",
       "        grad_fn=<ReshapeAliasBackward0>),\n",
       " 'predictions': tensor([[[[1., 1., 0.,  ..., 0., 0., 0.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 0.,  ..., 1., 0., 0.],\n",
       "           ...,\n",
       "           [1., 1., 0.,  ..., 1., 1., 1.],\n",
       "           [0., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [0., 0., 1.,  ..., 1., 0., 0.]]]]),\n",
       " 'accuracy': tensor(0.),\n",
       " 'precision': tensor(0.7952),\n",
       " 'f0point5': tensor(0.8108)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(img_width=192, in_channels=16, out_channels=1)\n",
    "# example image\n",
    "image = torch.randn(1, 16, 192, 192)  # input image\n",
    "targets = torch.randn(1, 1, 192, 192)  # input image\n",
    "targets = torch.abs(targets)\n",
    "\n",
    "output = model(image, targets)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2220],\n",
       "         [2.9907],\n",
       "         [1.2412],\n",
       "         [1.7058],\n",
       "         [0.6931]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate class weights based on the imbalance\n",
    "class_weights = torch.tensor([1.0, 5.0]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='none')\n",
    "\n",
    "# Example output and targets\n",
    "out = [[0.1], [0.2], [0.9], [0.9], [0]]\n",
    "tar = [[1], [1], [0], [1], [0]]\n",
    "output = torch.tensor([out], dtype=torch.float32)\n",
    "targets = torch.tensor([tar], dtype=torch.float32)\n",
    "loss_function(output, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9705)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate class weights based on the imbalance\n",
    "class_weights = torch.tensor([1.0, 5.0]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "# Example output and targets\n",
    "out = [[0.1], [0.2], [0.9], [0.9], [0]]\n",
    "tar = [[1], [1], [0], [1], [0]]\n",
    "output = torch.tensor([out], dtype=torch.float32)\n",
    "targets = torch.tensor([tar], dtype=torch.float32)\n",
    "loss_function(output, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1024]),\n",
       " tensor(0.8201, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " tensor(0.3237))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50Seg(in_channels=3, out_channels=1)\n",
    "x = torch.randn(16, 3, 192, 192)  # input image\n",
    "targets = torch.randn(16, 1024)  # input image\n",
    "\n",
    "# Normalize x to be between 0 and 1\n",
    "x = torch.sigmoid(x)\n",
    "\n",
    "# Clip targets to 0 and 1\n",
    "targets = torch.clamp(targets, 0, 1)\n",
    "\n",
    "\n",
    "output = model(x, targets)\n",
    "output['logits'].shape, output['loss'], output['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8094)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected value for the BCE loss if there is 1024 elements\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "y_true = torch.randn(1024, 1024) \n",
    "y_pred = torch.randn(1024, 1024) \n",
    "\n",
    "loss = loss_function(y_pred, y_true)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.1933)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected value for the BCE loss if there is 1024 elements that were all correctly classified\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "y_pred = torch.randn(1024*16, 1024) \n",
    "\n",
    "loss = loss_function(y_pred, y_pred)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit on 1 batch of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/450 [00:00<03:06,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1121, Accuracy: 0.4919, Precision: 0.0982, F0.5: 0.1171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/450 [00:03<02:16,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0670, Accuracy: 0.6594, Precision: 0.2008, F0.5: 0.2367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 21/450 [00:06<02:12,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0315, Accuracy: 0.7725, Precision: 0.2935, F0.5: 0.3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 30/450 [00:09<02:22,  2.95it/s][E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "  7%|▋         | 30/450 [00:09<02:19,  3.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb Cell 24\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m f0point5 \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mf0point5\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Update weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/viktorcikojevic/Insync/cikojevic.viktor%40gmail.com/Google%20Drive/Kaggle/vesuvius-challenge/models/model-2/model-dev.ipynb#X30sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = ResNet50Seg(in_channels=3, out_channels=1, class_one_weight=5)\n",
    "x = torch.randn(4, 3, 192, 192)  # input image\n",
    "# targets = random integers of shape (4, 1024) between 0 and 1\n",
    "targets = torch.randint(0, 2, (4, 1024)).float()\n",
    "# Make 80% of the targets to be 0\n",
    "targets[:, 0:819] = 0\n",
    "\n",
    "\n",
    "# Normalize x to be between 0 and 1\n",
    "x = torch.sigmoid(x)\n",
    "\n",
    "# Clip targets to 0 and 1\n",
    "targets = torch.clamp(targets, 0, 1)\n",
    "\n",
    "# optimizer \n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "n_iters = 450\n",
    "for i in tqdm(range(n_iters)):\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x, targets)\n",
    "    loss = output['loss']\n",
    "    precision = output['precision']\n",
    "    accuracy = output['accuracy']\n",
    "    f0point5 = output['f0point5']\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}, Precision: {precision.item():.4f}, F0.5: {f0point5.item():.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import avg_pool2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', device='cpu', cache_refresh_interval=None, cache_n_images=64):\n",
    "        \n",
    "        assert mode in ['train', 'test'], \"mode must be either 'train' or 'test'\"\n",
    "        \n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.cache_refresh_interval = cache_refresh_interval\n",
    "        self.cache_n_images = cache_n_images\n",
    "        self._load_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.volume_images)\n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Get the volume paths\n",
    "        self.volume_images = os.listdir(os.path.join(self.root, self.mode, 'volume'))\n",
    "        self.volume_images = [os.path.join(self.root, self.mode, 'volume', image) for image in self.volume_images]\n",
    "        \n",
    "        # Get label paths\n",
    "        self.label_images = os.listdir(os.path.join(self.root, self.mode, 'label'))\n",
    "        self.label_images = [os.path.join(self.root, self.mode, 'label', image) for image in self.label_images]\n",
    "        \n",
    "        # take self.cache_n_images random images from the dataset. They cannot be repeated\n",
    "        if self.cache_n_images is not None and self.cache_n_images < len(self.volume_images):\n",
    "            self.volume_images = np.random.choice(self.volume_images, size=self.cache_n_images, replace=False)\n",
    "            self.label_images = np.random.choice(self.label_images, size=self.cache_n_images, replace=False)\n",
    "        \n",
    "        # Load the data into memory\n",
    "        self.cached_data = []\n",
    "        for volume_image, label_image in zip(self.volume_images, self.label_images):\n",
    "            volume = np.load(volume_image)\n",
    "            label = np.load(label_image)\n",
    "            \n",
    "            # convert to torch tensors\n",
    "            volume = torch.tensor(volume, dtype=torch.float32)\n",
    "            label = torch.tensor(label, dtype=torch.float32)\n",
    "            \n",
    "            self.cached_data.append({\n",
    "                \"image\": volume,\n",
    "                \"targets\": label\n",
    "            }) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cached_data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.cached_data[item]\n",
    "    \n",
    "    def refresh_cache(self):\n",
    "        self._load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset and dataloader\n",
    "root = \"../../datasets/dataset-1/\"\n",
    "\n",
    "# Mac m1 device\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "dataset = ImageSegmentationDataset(root=root, mode='train', device=device, cache_refresh_interval=160, cache_n_images=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 384, 384, 16]), torch.Size([16, 384, 384, 1]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one batch\n",
    "batch = next(iter(dataloader))\n",
    "batch['image'].shape, batch['targets'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 100 iterations: 1.8975 seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's check how fast the dataloader is\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    batch = next(iter(dataloader))\n",
    "    # Refresh the cache \n",
    "    if i % dataset.cache_refresh_interval == 0:\n",
    "        dataset.refresh_cache()\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken for 100 iterations: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.1 ms ± 3.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.load(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/volume/0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 768, 64)\n",
      "(192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/volume/0.npy\")\n",
    "print(x.shape)\n",
    "x = x[::4, ::4, :3]\n",
    "print(x.shape)\n",
    "np.save(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/tmp/x.npy\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 µs ± 2.63 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.load(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/tmp/x.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 µs ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.load(\"/Users/viktorcikojevic/Insync/cikojevic.viktor@gmail.com/Google Drive/Kaggle/vesuvius-challenge/datasets/dataset-1/train/label/0.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: make it's best to generate dataset-1 by taking 4x4 avg_2d while creating the dataset. This will make the dataloader much much faster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'img_width': 384, 'in_channels': 16, 'out_channels': 1, 'init_features': 64, 'class_one_weight': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 64, 'num_steps': 100000, 'log_freq': 100, 'eval_steps': 100, 'dataset_dir': '../../datasets/dataset-1', 'device': 'cuda', 'cache_refresh_interval': 64, 'cache_n_images': 64, 'wandb': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Args loaded (dict form): {'img_width': 384, 'in_channels': 16, 'out_channels': 1, 'init_features': 64, 'class_one_weight': 1, 'seed': 42, 'learning_rate': 0.0001, 'batch_size': 64, 'num_steps': 100000, 'log_freq': 100, 'eval_steps': 100, 'dataset_dir': '../../datasets/dataset-1', 'device': 'cuda', 'cache_refresh_interval': 64, 'cache_n_images': 64, 'wandb': False}\n",
      "[LOG] img_width: 384\n",
      "[LOG] in_channels: 16\n",
      "[LOG] out_channels: 1\n",
      "[LOG] init_features: 64\n",
      "[LOG] class_one_weight: 1\n",
      "[LOG] seed: 42\n",
      "[LOG] learning_rate: 0.0001\n",
      "[LOG] batch_size: 64\n",
      "[LOG] num_steps: 100000\n",
      "[LOG] log_freq: 100\n",
      "[LOG] eval_steps: 100\n",
      "[LOG] dataset_dir: ../../datasets/dataset-1\n",
      "[LOG] device: cuda\n",
      "[LOG] cache_refresh_interval: 64\n",
      "[LOG] cache_n_images: 64\n",
      "[LOG] wandb: False\n",
      "Creating a model...\n",
      "Model created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/viktorcikojevic/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from dataloader import ImageSegmentationDataset\n",
    "from model import UNet\n",
    "from eval import evaluate\n",
    "\n",
    "\n",
    "\n",
    "print(f\"[LOG] Args loaded (dict form): {args}\")\n",
    "for key, value in args.items():\n",
    "    print(f\"[LOG] {key}: {value}\")\n",
    "\n",
    "# Set the device\n",
    "if args['device'] == 'cuda':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageSegmentationDataset(root=args['dataset_dir'], mode='train', device=device, cache_refresh_interval=args['cache_refresh_interval'], cache_n_images=args['cache_n_images'])\n",
    "test_dataset = ImageSegmentationDataset(root=args['dataset_dir'], mode='test', device=device, cache_refresh_interval=args['cache_refresh_interval'], cache_n_images=args['cache_n_images'])\n",
    "\n",
    "train_dataloader =  DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "test_dataloader =  DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "# Create a model\n",
    "print(\"Creating a model...\")\n",
    "model = UNet(img_width=args['img_width'], in_channels=args['in_channels'], out_channels=args['out_channels'], init_features=args['init_features'], class_one_weight=args['class_one_weight'])\n",
    "print(\"Model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up wandb\n",
    "if args['wandb']:\n",
    "    wandb.login(key=\"YOUR_WANDB_API_KEY\")\n",
    "    wandb.init(project='vesuvius-challenge', config=args)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "# Initialize the metrics dictionary\n",
    "metrics = {\n",
    "    \"loss\": [],\n",
    "    \"logits\": [],\n",
    "    \"predictions\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"f0point5\": []\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame(columns=[\"step\"] + list(metrics.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384, 384, 16]), torch.Size([64, 384, 384, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "image, labels = batch['image'], batch['targets']\n",
    "image.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 384, 384])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.permute(0, 3, 1, 2).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
