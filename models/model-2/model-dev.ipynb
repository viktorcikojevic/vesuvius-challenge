{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Takes as input the (768, 768, 64) images from `dataset-1`.\n",
    "- It takes 4^3 average across the 2D, and average across the depth\n",
    "- This results in a (192, 192, 16) volume.\n",
    "- I have a image that is of shape (1, 192, 192, 16). Divide the image and the labels into 32 patches: (1, 8*24, 8*24, 16) → (24, 24, 1024) → (576, 1024). \n",
    "- Pass the image into a transformer architecture with 6 layers, which will return (576, 1) tensor, which corresponds to probabilities of pixel occuring. You should also pass the “positional embeddings”, which should be of dimension 1024."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4^3 3D average \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def avg_3d(volume):\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    volume_tensor = torch.tensor(volume, dtype=torch.float32)\n",
    "\n",
    "    # Add batch and channel dimensions to the tensor\n",
    "    volume_tensor = volume_tensor.permute(0, 3, 1, 2)  # Reorder dimensions to (batch, channels, height, width)\n",
    "\n",
    "    # Create the 3D average pooling layer with the appropriate kernel size and stride values\n",
    "    avg_pool = nn.AvgPool3d(kernel_size=4, stride=4, padding=0)\n",
    "\n",
    "    # Apply the average pooling layer to the input tensor\n",
    "    with torch.no_grad():\n",
    "        filtered_volume_tensor = avg_pool(volume_tensor)\n",
    "\n",
    "    # Convert the output tensor back to a numpy array\n",
    "    filtered_volume = filtered_volume_tensor.permute(0, 2, 3, 1)  # Reorder dimensions back to (batch, height, width, channels)\n",
    "    \n",
    "    return filtered_volume\n",
    "\n",
    "\n",
    "# Example volume\n",
    "volume = np.random.rand(1, 768, 768, 64)\n",
    "volume_avgd = avg_3d(volume)\n",
    "# %time avg_3d(volume) # 85 ms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 192, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_avgd.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take random 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 192, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_avgd_new = volume_avgd.permute(3, 1, 2, 0)\n",
    "# take 3 random numbers between 0 and 16, without replacement\n",
    "random_numbers = np.random.choice(16, 3, replace=False)\n",
    "volume_avgd_new = volume_avgd_new[random_numbers, :, :, :]\n",
    "# Repermute\n",
    "volume_avgd_new = volume_avgd_new.permute(3, 1, 2, 0)\n",
    "volume_avgd_new.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 576, 1024])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def reshape_img(image):\n",
    "    # Calculate the size of each patch\n",
    "    patch_size = 24\n",
    "\n",
    "    B, H, W, C = image.shape\n",
    "    image = image.reshape(B, H // patch_size, patch_size, W // patch_size, patch_size, C) # (B, 8, 24, 8, 24, 16)\n",
    "    image = image.permute(0, 2, 4, 1, 3, 5) # (B, 24, 24, 8, 8, 16)\n",
    "    image = image.reshape(B, patch_size, patch_size, -1) # (B, 24, 24, 1024)\n",
    "    image = image.reshape(B, -1, 1024) # (B, 576, 1024)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Example image\n",
    "image = np.random.rand(2, 192, 192, 16)\n",
    "image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "\n",
    "image = reshape_img(image)\n",
    "\n",
    "print(image.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric competition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0point5_score(output, target):\n",
    "    # Flatten the output and target tensors\n",
    "    output = output.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    # Convert the output to binary values 0 and 1\n",
    "    output = (output > 0.5).float()\n",
    "    \n",
    "    # Calculate the precision and recall\n",
    "    tp = torch.sum(output * target)\n",
    "    fp = torch.sum(output * (1 - target))\n",
    "    fn = torch.sum((1 - output) * target)\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    \n",
    "    # Calculate the F0.5 score\n",
    "    beta = 0.5\n",
    "    f0point5 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-7)\n",
    "    return f0point5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: ResNet50Seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet50Seg(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, class_one_weight=1):\n",
    "        super(ResNet50Seg, self).__init__()\n",
    "        \n",
    "        self.class_one_weight = class_one_weight\n",
    "        \n",
    "        # Load the pre-trained ResNet50 model\n",
    "        self.resnet50 = models.resnet50()\n",
    "        \n",
    "        # Replace the final layer to output 1024 channel instead of 1000\n",
    "        self.resnet50.fc = nn.Linear(2048, 1024)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.resnet50(x)\n",
    "        # Apply sigmoid\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        out = {}\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            precision = None\n",
    "            accuracy = None\n",
    "            f0point5 = None\n",
    "        else:\n",
    "            # Calculate the loss\n",
    "            \n",
    "            # Calculate class weights based on the imbalance\n",
    "            class_weights = torch.tensor([1.0, self.class_one_weight]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "            # Instantiate the loss function\n",
    "            loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "            \n",
    "            # Flatten the targets tensor\n",
    "            targets = targets.reshape(-1, 1024)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_function(x, targets)\n",
    "            \n",
    "            # Calculate the accuracy: number of correctly predicted pixel / total number of pixels\n",
    "            # Convert the predictions to binary values 0 and 1\n",
    "            predictions = (x > 0.5).float()\n",
    "            # Calculate the accuracy\n",
    "            accuracy = (predictions == targets).float().mean()\n",
    "            \n",
    "            # Calculate the precision\n",
    "            tp = torch.sum(predictions * targets)   \n",
    "            fp = torch.sum(predictions * (1 - targets))\n",
    "            fn = torch.sum((1 - predictions) * targets)\n",
    "            precision = tp / (tp + fp + 1e-7)\n",
    "            \n",
    "            # Calculate F0.5 score\n",
    "            f0point5 = f0point5_score(predictions, targets) \n",
    "        \n",
    "        out = {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": x,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"f0point5\": f0point5\n",
    "        }\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50Seg(in_channels=3, out_channels=1)\n",
    "x = torch.randn(1, 3, 192, 192)  # input image\n",
    "output = model(x)\n",
    "output['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.606208"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of 1e6 parameters\n",
    "sum(p.numel() for p in model.parameters()) / 1e6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2220],\n",
       "         [2.9907],\n",
       "         [1.2412],\n",
       "         [1.7058],\n",
       "         [0.6931]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate class weights based on the imbalance\n",
    "class_weights = torch.tensor([1.0, 5.0]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='none')\n",
    "\n",
    "# Example output and targets\n",
    "out = [[0.1], [0.2], [0.9], [0.9], [0]]\n",
    "tar = [[1], [1], [0], [1], [0]]\n",
    "output = torch.tensor([out], dtype=torch.float32)\n",
    "targets = torch.tensor([tar], dtype=torch.float32)\n",
    "loss_function(output, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9705)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate class weights based on the imbalance\n",
    "class_weights = torch.tensor([1.0, 5.0]) # weight 1 for class 0, weight 5 for class 1\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "# Example output and targets\n",
    "out = [[0.1], [0.2], [0.9], [0.9], [0]]\n",
    "tar = [[1], [1], [0], [1], [0]]\n",
    "output = torch.tensor([out], dtype=torch.float32)\n",
    "targets = torch.tensor([tar], dtype=torch.float32)\n",
    "loss_function(output, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1024]),\n",
       " tensor(0.8191, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " tensor(0.3298))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50Seg(in_channels=3, out_channels=1)\n",
    "x = torch.randn(16, 3, 192, 192)  # input image\n",
    "targets = torch.randn(16, 1024)  # input image\n",
    "\n",
    "# Normalize x to be between 0 and 1\n",
    "x = torch.sigmoid(x)\n",
    "\n",
    "# Clip targets to 0 and 1\n",
    "targets = torch.clamp(targets, 0, 1)\n",
    "\n",
    "\n",
    "output = model(x, targets)\n",
    "output['logits'].shape, output['loss'], output['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8055)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected value for the BCE loss if there is 1024 elements\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "y_true = torch.randn(1024, 1024) \n",
    "y_pred = torch.randn(1024, 1024) \n",
    "\n",
    "loss = loss_function(y_pred, y_true)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.1928)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected value for the BCE loss if there is 1024 elements that were all correctly classified\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=class_weights[1], reduction='mean')\n",
    "\n",
    "y_pred = torch.randn(1024*16, 1024) \n",
    "\n",
    "loss = loss_function(y_pred, y_pred)\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit on 1 batch of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/450 [00:00<02:41,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1170, Accuracy: 0.5149, Precision: 0.1084, F0.5: 0.1288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/450 [00:03<02:03,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0715, Accuracy: 0.6597, Precision: 0.1951, F0.5: 0.2289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 21/450 [00:06<01:58,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0357, Accuracy: 0.7729, Precision: 0.2938, F0.5: 0.3386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 31/450 [00:08<01:58,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9999, Accuracy: 0.8662, Precision: 0.4292, F0.5: 0.4812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 41/450 [00:11<01:55,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9654, Accuracy: 0.9170, Precision: 0.5525, F0.5: 0.6062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 51/450 [00:14<01:52,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9338, Accuracy: 0.9387, Precision: 0.6254, F0.5: 0.6760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 61/450 [00:17<01:49,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9062, Accuracy: 0.9487, Precision: 0.6661, F0.5: 0.7138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 71/450 [00:20<02:03,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8832, Accuracy: 0.9500, Precision: 0.6715, F0.5: 0.7187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 81/450 [00:23<01:49,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8647, Accuracy: 0.9502, Precision: 0.6726, F0.5: 0.7197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 91/450 [00:26<01:50,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8500, Accuracy: 0.9500, Precision: 0.6715, F0.5: 0.7187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 101/450 [00:29<01:45,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8385, Accuracy: 0.9502, Precision: 0.6726, F0.5: 0.7197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 111/450 [00:32<01:41,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8299, Accuracy: 0.9502, Precision: 0.6726, F0.5: 0.7197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 121/450 [00:35<01:35,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8234, Accuracy: 0.9502, Precision: 0.6726, F0.5: 0.7197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 131/450 [00:38<01:34,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8183, Accuracy: 0.9521, Precision: 0.6813, F0.5: 0.7277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 141/450 [00:41<01:30,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8143, Accuracy: 0.9529, Precision: 0.6846, F0.5: 0.7307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 151/450 [00:44<01:26,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8110, Accuracy: 0.9565, Precision: 0.7018, F0.5: 0.7463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 161/450 [00:47<01:28,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8081, Accuracy: 0.9607, Precision: 0.7224, F0.5: 0.7649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 171/450 [00:50<01:18,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8056, Accuracy: 0.9648, Precision: 0.7442, F0.5: 0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 181/450 [00:53<01:15,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8034, Accuracy: 0.9675, Precision: 0.7591, F0.5: 0.7975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 191/450 [00:55<01:14,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8014, Accuracy: 0.9705, Precision: 0.7759, F0.5: 0.8123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 201/450 [00:58<01:16,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7997, Accuracy: 0.9736, Precision: 0.7951, F0.5: 0.8290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 211/450 [01:01<01:08,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7982, Accuracy: 0.9756, Precision: 0.8073, F0.5: 0.8397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 221/450 [01:05<01:21,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7968, Accuracy: 0.9780, Precision: 0.8232, F0.5: 0.8534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 231/450 [01:08<01:07,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7956, Accuracy: 0.9797, Precision: 0.8347, F0.5: 0.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 241/450 [01:11<01:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7946, Accuracy: 0.9819, Precision: 0.8499, F0.5: 0.8762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 251/450 [01:14<00:59,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7938, Accuracy: 0.9827, Precision: 0.8551, F0.5: 0.8806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 261/450 [01:17<00:54,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7931, Accuracy: 0.9836, Precision: 0.8621, F0.5: 0.8866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 271/450 [01:20<00:51,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7925, Accuracy: 0.9841, Precision: 0.8657, F0.5: 0.8896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 281/450 [01:22<00:48,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7920, Accuracy: 0.9851, Precision: 0.8729, F0.5: 0.8957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 291/450 [01:25<00:45,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7915, Accuracy: 0.9858, Precision: 0.8784, F0.5: 0.9003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 301/450 [01:29<00:52,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7911, Accuracy: 0.9866, Precision: 0.8840, F0.5: 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 311/450 [01:32<00:41,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7908, Accuracy: 0.9871, Precision: 0.8877, F0.5: 0.9081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 321/450 [01:35<00:38,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7905, Accuracy: 0.9873, Precision: 0.8896, F0.5: 0.9097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 331/450 [01:38<00:35,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7902, Accuracy: 0.9880, Precision: 0.8953, F0.5: 0.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 341/450 [01:41<00:32,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7900, Accuracy: 0.9885, Precision: 0.8991, F0.5: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 351/450 [01:44<00:31,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7898, Accuracy: 0.9885, Precision: 0.8991, F0.5: 0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 361/450 [01:47<00:25,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7896, Accuracy: 0.9888, Precision: 0.9011, F0.5: 0.9193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 371/450 [01:50<00:22,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7894, Accuracy: 0.9893, Precision: 0.9050, F0.5: 0.9225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 381/450 [01:53<00:19,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7892, Accuracy: 0.9895, Precision: 0.9069, F0.5: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 391/450 [01:56<00:17,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7891, Accuracy: 0.9895, Precision: 0.9069, F0.5: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 401/450 [01:59<00:14,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7889, Accuracy: 0.9895, Precision: 0.9069, F0.5: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 411/450 [02:02<00:11,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7888, Accuracy: 0.9897, Precision: 0.9089, F0.5: 0.9258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 421/450 [02:05<00:08,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7887, Accuracy: 0.9905, Precision: 0.9148, F0.5: 0.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 431/450 [02:08<00:05,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7886, Accuracy: 0.9905, Precision: 0.9148, F0.5: 0.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 441/450 [02:10<00:02,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7885, Accuracy: 0.9905, Precision: 0.9148, F0.5: 0.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [02:13<00:00,  3.37it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = ResNet50Seg(in_channels=3, out_channels=1, class_one_weight=5)\n",
    "x = torch.randn(4, 3, 192, 192)  # input image\n",
    "# targets = random integers of shape (4, 1024) between 0 and 1\n",
    "targets = torch.randint(0, 2, (4, 1024)).float()\n",
    "# Make 80% of the targets to be 0\n",
    "targets[:, 0:819] = 0\n",
    "\n",
    "\n",
    "# Normalize x to be between 0 and 1\n",
    "x = torch.sigmoid(x)\n",
    "\n",
    "# Clip targets to 0 and 1\n",
    "targets = torch.clamp(targets, 0, 1)\n",
    "\n",
    "# optimizer \n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "n_iters = 450\n",
    "for i in tqdm(range(n_iters)):\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x, targets)\n",
    "    loss = output['loss']\n",
    "    precision = output['precision']\n",
    "    accuracy = output['accuracy']\n",
    "    f0point5 = output['f0point5']\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}, Precision: {precision.item():.4f}, F0.5: {f0point5.item():.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import avg_pool2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', device='cpu', cache_refresh_interval=None, cache_n_images=64):\n",
    "        \n",
    "        assert mode in ['train', 'test'], \"mode must be either 'train' or 'test'\"\n",
    "        \n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.cache_refresh_interval = cache_refresh_interval\n",
    "        self.cache_n_images = cache_n_images\n",
    "        self._load_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.volume_images)\n",
    "    \n",
    "    # def __getitem__(self, item):\n",
    "        \n",
    "    #     volume = np.load(self.volume_images[item])\n",
    "    #     label = np.load(self.label_images[item])\n",
    "        \n",
    "    #     # convert to torch tensors\n",
    "    #     volume = torch.tensor(volume, dtype=torch.float32)\n",
    "    #     label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "    #     # send to device\n",
    "    #     volume = volume.to(self.device)\n",
    "    #     label = label.to(self.device)\n",
    "        \n",
    "    #     # unsqueeze the channel dimension\n",
    "    #     label = label.unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "    #     # take one random number from 0 to 20, 20 to 35, 35 to 50, 50 to 64\n",
    "    #     idx_1 = np.random.randint(0, 20)\n",
    "    #     idx_2 = np.random.randint(20, 35)\n",
    "    #     idx_3 = np.random.randint(35, 50)\n",
    "    #     indices = [idx_1, idx_2, idx_3]\n",
    "    #     volume = volume[:, :, indices]\n",
    "        \n",
    "    #     # Apply 2D average pooling to the volume and label tensors\n",
    "    #     # Average each channel separately\n",
    "    #     volume = volume.permute(2, 0, 1)  # Permute dimensions for avg_pool2d\n",
    "    #     volume = avg_pool2d(volume, kernel_size=4, stride=4)\n",
    "    #     volume = volume.permute(1, 2, 0)  # Permute dimensions back\n",
    "        \n",
    "    #     label = label.permute(2, 0, 1)  # Permute dimensions for avg_pool2d\n",
    "    #     label = avg_pool2d(label, kernel_size=4, stride=4)\n",
    "    #     label = label.permute(1, 2, 0)  # Permute dimensions back\n",
    "        \n",
    "    #     return {\n",
    "    #         \"image\": volume,\n",
    "    #         \"targets\": label\n",
    "    #     }\n",
    "       \n",
    "    def _load_data(self):\n",
    "        # Get the volume paths\n",
    "        self.volume_images = os.listdir(os.path.join(self.root, self.mode, 'volume'))\n",
    "        self.volume_images = [os.path.join(self.root, self.mode, 'volume', image) for image in self.volume_images]\n",
    "        \n",
    "        # Get label paths\n",
    "        self.label_images = os.listdir(os.path.join(self.root, self.mode, 'label'))\n",
    "        self.label_images = [os.path.join(self.root, self.mode, 'label', image) for image in self.label_images]\n",
    "        \n",
    "        # take self.cache_n_images random images from the dataset. They cannot be repeated\n",
    "        if self.cache_n_images is not None and self.cache_n_images < len(self.volume_images):\n",
    "            self.volume_images = np.random.choice(self.volume_images, size=self.cache_n_images, replace=False)\n",
    "            self.label_images = np.random.choice(self.label_images, size=self.cache_n_images, replace=False)\n",
    "        \n",
    "        # Load the data into memory\n",
    "        self.cached_data = []\n",
    "        for volume_image, label_image in zip(self.volume_images, self.label_images):\n",
    "            volume = np.load(volume_image)\n",
    "            label = np.load(label_image)\n",
    "            \n",
    "            # convert to torch tensors\n",
    "            volume = torch.tensor(volume, dtype=torch.float32)\n",
    "            label = torch.tensor(label, dtype=torch.float32)\n",
    "            \n",
    "            # unsqueeze the channel dimension\n",
    "            label = label.unsqueeze(-1)\n",
    "            \n",
    "            self.cached_data.append({\n",
    "                \"image\": volume,\n",
    "                \"targets\": label\n",
    "            }) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cached_data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return self.cached_data[item]\n",
    "    \n",
    "    def refresh_cache(self):\n",
    "        self._load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset and dataloader\n",
    "root = \"../../datasets/dataset-1/\"\n",
    "\n",
    "# Mac m1 device\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "dataset = ImageSegmentationDataset(root=root, mode='train', device=device, cache_refresh_interval=160, cache_n_images=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 768, 768, 64]), torch.Size([5, 768, 768, 1]))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one batch\n",
    "batch = next(iter(dataloader))\n",
    "batch['image'].shape, batch['targets'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 100 iterations: 11.6429 seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's check how fast the dataloader is\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    batch = next(iter(dataloader))\n",
    "    # Refresh the cache \n",
    "    if i % dataset.cache_refresh_interval == 0:\n",
    "        dataset.refresh_cache()\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken for 100 iterations: {end - start:.4f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
